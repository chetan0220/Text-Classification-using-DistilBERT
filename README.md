# Text-Classification-using-DistilBERT
The project leverages DistilBERT with transfer learning to classify e-commerce text into "Electronics," "Household," "Books," and "Clothing & Accessories." By fine-tuning this efficient, pre-trained transformer model, it achieves high accuracy with minimal training data, optimizing speed and reducing computational needs for large-scale NLP categorization.

[DATASET](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)

---
---

# Visualization
![image](https://github.com/user-attachments/assets/78919ee1-20d0-4368-b53f-63d621aff7ce) <br>

---

![image](https://github.com/user-attachments/assets/a4b26668-aaf8-43cc-966f-305ea46bbd89) <br>

---

![image](https://github.com/user-attachments/assets/9a2578d0-fe73-4605-bf5e-9c446c279bd1) <br>

---
---

# Results
![image](https://github.com/user-attachments/assets/a2bb068c-7a3c-434e-8447-789ed9cef1af)

---
---
